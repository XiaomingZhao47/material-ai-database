{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiaomingZhao47/material-ai-database/blob/main/HelloCUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq4JwmB9Tdua"
      },
      "source": [
        "# Hello CUDA\n",
        "\n",
        "A CPU (Central Processing Unit) and a GPU (Graphics Processing Unit) are both types of processors, but they are designed for different types of workloads.\n",
        "\n",
        "A CPU is designed to handle a wide variety of tasks, such as running the operating system, executing application programs, and performing complex calculations. A CPU typically has a small number of cores, which are optimized for sequential execution of instructions, and a large cache memory to hold frequently used data.\n",
        "\n",
        "On the other hand, a GPU is designed to handle large amounts of data in parallel. A GPU has a large number of cores, which are optimized for executing the same instruction on multiple pieces of data in parallel. GPUs also have a large amount of memory called VRAM (Video RAM) that holds the data that the cores are working on.\n",
        "\n",
        "In general, a CPU is better suited for tasks that require sequential execution of instructions and complex branching operations, while a GPU is better suited for tasks that can be parallelized, such as image processing, scientific simulations, and machine learning.\n",
        "\n",
        "When a program is executed on a CPU it's called serial execution, while when a program is executed on a GPU it's called parallel execution. A GPU can perform the same operation on thousands of data points at the same time, this means that a GPU can perform certain tasks much faster than a CPU, specially when the task can be parallelized.\n",
        "\n",
        "In recent years, with the rise of deep learning and AI, the use of GPUs for general purpose computation (GPGPU) has increased, and many libraries and frameworks for deep learning such as TensorFlow, PyTorch, Caffe and others are optimized to run on GPUs to accelerate the computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xFuEfjGbOJ7L",
        "outputId": "585c0c2a-19d4-4c58-af22-ce525ada5948",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "# make sure CUDA is installed\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p2X708J3OMBt",
        "outputId": "336fc085-9342-4cee-ae94-e7367365c6b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 30 18:58:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# make sure you have a GPU runtime (if this fails go to runtime -> change runtime type)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qgvq2y2oOVWr",
        "outputId": "f898c0ce-91c2-42f6-ed08-8168a1d2344f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  8758  100  8758    0     0   120k      0 --:--:-- --:--:-- --:--:--  122k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7891  100  7891    0     0  22237      0 --:--:-- --:--:-- --:--:-- 22228\n"
          ]
        }
      ],
      "source": [
        "# Install some magic to run and save .cpp programs\n",
        "!curl -o ./cpu_runner.py https://raw.githubusercontent.com/COSC-169-23-F25/helpers/main/cpu_runner.py\n",
        "%load_ext cpu_runner\n",
        "\n",
        "# Install some magic to run and save .cu C++ CUDA programs\n",
        "!curl -o ./gpu_runner.py https://raw.githubusercontent.com/COSC-169-23-F25/helpers/main/gpu_runner.py\n",
        "%load_ext gpu_runner\n",
        "\n",
        "# to learn about how to do more fancy things with CUDA using this API see:\n",
        "# https://nvcc4jupyter.readthedocs.io/en/latest/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My-wnhU8TVFu"
      },
      "source": [
        "The CUDA programming model is based on the concept of dividing a computation into small, independently executing units called threads. These threads are organized into groups called thread blocks. A thread block is a collection of threads that can cooperatively share data through shared memory and synchronize their execution through barrier instructions.\n",
        "\n",
        "**Threads**: A thread is the basic unit of execution in CUDA. Each thread has its own program counter, registers, and stack, and can execute its own instructions independently of other threads. In CUDA, threads are organized into a one, two or three-dimensional grid of thread blocks, and each thread is identified by a unique thread index within its block.\n",
        "\n",
        "**Thread Blocks**: A thread block is a collection of threads that can cooperatively share data through shared memory and synchronize their execution through barrier instructions. Thread blocks are organized into a one, two or three-dimensional grid of thread blocks, and each thread block is identified by a unique block index within the grid. Threads within a block can communicate through shared memory and synchronize their execution by executing barrier instructions.\n",
        "\n",
        "Thread blocks are launched on the GPU in a kernel launch call, the kernel launch syntax `<<<dimGrid, dimBlock>>>` is used to specify the number of blocks and the number of threads in each block.\n",
        "\n",
        "Threads within a block are executed in parallel, and each thread can be thought of as a separate execution unit that can run its own instruction stream. Threads in different blocks can also run in parallel and are executed independently of one another.\n",
        "\n",
        "In CUDA, the programmer has to decide on the number of blocks and the number of threads per block, this is called the thread hierarchy. The thread hierarchy is used to balance the computation and memory accesses with the number of available CUDA cores and the amount of shared memory. Choosing the right thread hierarchy is crucial for the performance of the CUDA program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_-MQvIPQ-S0"
      },
      "source": [
        "## CUDA Hello World\n",
        "\n",
        "Here's a breakdown of the key elements of this example:\n",
        "\n",
        "`#include <cuda_runtime.h>`: This is the header file for the CUDA runtime library, which provides the functionality needed to run CUDA kernels on the GPU.\n",
        "\n",
        "`__global__ void helloWorldKernel()`: This function is a CUDA kernel, which runs on the GPU. The `__global__` keyword indicates that this function is a kernel that can be executed on the GPU. Similarly the `__host__` before the main function says that it runs on the CPU. There is also a `__device__` specifier you can use for GPU helper functions that ONLY get called by other GPU functions.\n",
        "\n",
        "`helloWorldKernel<<<1, 1>>>()`: This line launches the kernel on the GPU. The triple angle brackets `<<< >>>` is called the kernel launch syntax and it's used to specify the configuration of the kernel launch. In this case, we're launching 1 block of 1 thread.\n",
        "\n",
        "`cudaDeviceSynchronize()`: This function waits for all the work in the GPU to finish before it allows the CPU code to continue. This is to ensure that the kernel has finished executing before the program exits. This is VERY VERY SLOW and so you only want to synchronize the whole device when you absolutely need to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WvgtytyrTDK_",
        "outputId": "faf88655-5749-4cb0-fe89-9755da988b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.5\n",
            "[nvcc build]  nvcc -arch=sm_75 -o hello_world hello_world.cu\n",
            "[run]         ./hello_world\n",
            "Hello World!\n"
          ]
        }
      ],
      "source": [
        "%%gpurun -n hello_world.cu\n",
        "#include <cstdio>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// This function runs on the GPU\n",
        "__global__ void helloWorldKernel() {\n",
        "    printf(\"Hello World!\\n\");\n",
        "}\n",
        "\n",
        "// This function runs on the CPU\n",
        "__host__\n",
        "int main() {\n",
        "    // Launch the kernel on the GPU\n",
        "    helloWorldKernel<<<1, 1>>>();\n",
        "\n",
        "    // Wait for the kernel to finish executing\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5g6b6iXQ1Og"
      },
      "source": [
        "## Adding in Memory\n",
        "\n",
        "Lets now explore a CUDA C++ program that adds two arrays of integers.\n",
        "\n",
        "The program defines a kernel function called `add` that takes four arguments: two input arrays, `a` and `b`, one output array, `c`, and the number of elements in the arrays, `n`. The kernel function uses the built-in `threadIdx.x` variable to determine the index of the thread that is executing the kernel. Each thread adds the corresponding element from `a` and `b` and stores the result in `c`.\n",
        "\n",
        "In order to handle the fact that there is seperate memory on the **device** (GPU) and **host** (CPU) we need to use the GPU analog to `malloc`, `cudaMalloc`. Do note that while `cudaMalloc` is used on the host it returns a pointer to device memory and it will segfault if you dereference that point in a host function.\n",
        "\n",
        "With that said, in the main function, we define three arrays, `h_a`, `h_b` and `h_c` (with `h_` indicating it is host memory by convention) and initialize the arrays with some values. Next, we allocate space for three device arrays, `d_a`, `d_b`, and `d_c` using the `cudaMalloc` function.\n",
        "\n",
        "We then use the `cudaMemcpy` function to copy the data from the host arrays to the device arrays. This is critical as again the memory is completely seperate and we need to ship it over the I/O Bus. As you might imagine this is very slow! And so the idea with using the GPU efficiently is to ship AS MUCH AS POSSIBLE at once!\n",
        "\n",
        "To invoke the kernel, we use the `add<<<1, n>>>(d_a, d_b, d_c, n);` syntax, where the number of blocks and the number of threads per block are specified by `1` and `n`, respectively.\n",
        "\n",
        "We then use `cudaMemcpy` again to copy the data from the device array back to the host array.\n",
        "\n",
        "Finally, we print the contents of the host array and free the device memory using `cudaFree`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5CmlGfLqTE9Y",
        "outputId": "e5b01bab-ea8a-4620-f6f5-bcd6be4a217c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.5\n",
            "[nvcc build]  nvcc -arch=sm_75 -o add add.cu\n",
            "[run]         ./add\n",
            "0 3 6 9 12 15 18 21 24 27\n"
          ]
        }
      ],
      "source": [
        "%%gpurun -n add.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__\n",
        "void add(int *d_a, int *d_b, int *d_c, int n){\n",
        "    // this if statement makes sure we don't go out of bounds of the array\n",
        "    int i = threadIdx.x;\n",
        "    if (i < n){\n",
        "        d_c[i] = d_a[i] + d_b[i];\n",
        "    }\n",
        "    // a better way to do this would be with a for loop in case it is not\n",
        "    //   launched with enough threads to cover all \"n\" computations\n",
        "    // for (int i = threadIdx.x; i < n; i += blockDim.x){\n",
        "    //     d_c[i] = d_a[i] + d_b[i];\n",
        "    // }\n",
        "}\n",
        "\n",
        "__host__\n",
        "int main(){\n",
        "\n",
        "    // constants and host memory\n",
        "    const int n = 10;\n",
        "    int h_a[n], h_b[n], h_c[n];\n",
        "\n",
        "    // Initialize the arrays\n",
        "    for (int i = 0; i < n; i++){\n",
        "        h_a[i] = i;\n",
        "        h_b[i] = i * 2;\n",
        "    }\n",
        "\n",
        "    // device memory\n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, n * sizeof(int));\n",
        "    cudaMalloc(&d_b, n * sizeof(int));\n",
        "    cudaMalloc(&d_c, n * sizeof(int));\n",
        "\n",
        "    // copy from host to device\n",
        "    cudaMemcpy(d_a, h_a, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // run the kernel\n",
        "    add<<<1, n>>>(d_a, d_b, d_c, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // copy the memory back\n",
        "    cudaMemcpy(h_c, d_c, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Print the results\n",
        "    for (int i = 0; i < n; i++){\n",
        "        std::cout << h_c[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    // free the allocated memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGx-pJ8BrLUs"
      },
      "source": [
        "## Synchronizations\n",
        "Synchronization is crucial in CUDA C++ to ensure that threads within a GPU kernel or a block coordinate their activities properly. Without synchronization, concurrent threads can access and modify shared data in an unpredictable and inconsistent manner, leading to incorrect results and even program crashes. Two key concepts for this may occur are as follows:\n",
        "\n",
        "1. *Data Consistency*: In parallel computing, multiple threads often work together to process data. Without synchronization, these threads may access the same data simultaneously, leading to data races and inconsistent results.\n",
        "\n",
        "2. *Order of Execution*: CUDA threads are executed asynchronously and can be scheduled in any order by the GPU. Synchronization ensures that threads follow a specific order of execution, which is crucial for algorithms that depend on specific thread coordination.\n",
        "\n",
        "In the following code examples we'll show why using synchronization is neccessary!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU98R0NuJqw1"
      },
      "source": [
        "Lets revisit our Hello World example, but this time with the `cudaDeviceSynchronize();` commented out! Run the code, what happens?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyBxpg_kJst2"
      },
      "outputs": [],
      "source": [
        "%%gpurun -n hello_world2.cu\n",
        "#include <cstdio>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// This function runs on the GPU\n",
        "__global__ void helloWorldKernel() {\n",
        "    printf(\"Hello World!\\n\");\n",
        "}\n",
        "\n",
        "// This function runs on the CPU\n",
        "__host__\n",
        "int main() {\n",
        "    // Launch the kernel on the GPU\n",
        "    helloWorldKernel<<<1, 1>>>();\n",
        "\n",
        "    // Wait for the kernel to finish executing\n",
        "    // cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv4NxGImKBo9"
      },
      "source": [
        "Where did the output go? Well what happend is that the kernel launched and while it was running on the GPU, the CPU code kept going and hit the return and exited the program! So the process excited before the `printf` could occur! Try to uncomment that line and run it again. Now you should see the text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdUnqGrsvzIi"
      },
      "source": [
        "## Your Turn!\n",
        "\n",
        "Can you finish the implementation below to design code to preform a vector reduction? Aka summing an array of length `n` of all `1`s using `1` block of `n` threads.\n",
        "\n",
        "*Note: the code assumes the solution will be put in `d_vals[0]`.*\n",
        "\n",
        "*Hint: I made `n` a power of 2 to keep things simple for you in terms of bookkeeping. But you probably want to keep track of how many threads you want to use or how many cells offset you want to sum etc. as you loop!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hskXht4CvjuS"
      },
      "outputs": [],
      "source": [
        "%%gpurun -n reduce.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// sums d_vals and places the result in d_vals[0]\n",
        "__global__\n",
        "void vectorReduction(int *d_vals, int n) {\n",
        "    /**********************\n",
        "     *                    *\n",
        "     *        TODO        *\n",
        "     *                    *\n",
        "     **********************/\n",
        "}\n",
        "\n",
        "__host__\n",
        "int main() {\n",
        "    const int n = 64;\n",
        "\n",
        "    // initialize the values to 1 and copy onto the GPU\n",
        "    int h_vals[n]; for(int i = 0; i < n; i++){h_vals[i] = 1;}\n",
        "    int *d_vals; cudaMalloc((void**)&d_vals, n*sizeof(int));\n",
        "    cudaMemcpy(d_vals, h_vals, n*sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // run the kernel\n",
        "    vectorReduction<<<1, n>>>(d_vals, n); // Launch kernel with n threads\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // copy the result back\n",
        "    cudaMemcpy(h_vals, d_vals, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // display the result\n",
        "    std::cout << h_vals[0] << \" \";\n",
        "\n",
        "    cudaFree(d_vals);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H7iEuUPyCBD"
      },
      "source": [
        "**Solution Below -- Don't Scroll Down Until You Are Done**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "Note that what we are doing here is taking half of the size of the array (`n/2`) and adding that much offset in parallel over threads. Then we are dividing the size by two for the next step and exiting when we are at size 1! Copy and paste the solution in above and see what happens when you vary `n`? For example at `64` you should see it return `64` but at 70 it should also return `64`. Why? How could you fix it? Try to think about what you would want to modify!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgKWiCpEz0WO"
      },
      "outputs": [],
      "source": [
        "__global__\n",
        "void vectorReduction(int *d_vals, int n) {\n",
        "    while(n > 1){\n",
        "        for(int i = threadIdx.x; i < n/2; i += blockDim.x){\n",
        "            d_vals[i] += d_vals[i + n/2];\n",
        "        }\n",
        "        n /= 2;\n",
        "        __syncthreads();\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJk5v5Sv0QbO"
      },
      "source": [
        "**Solution Below -- Don't Scroll Down Until You Are Done**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "We are being foiled by integer math! `n/2` will allways round down to the nearest whole number! So sometimes we miss `1` that we need to add back in when `n` is odd (note that `n%2` is the remainder operator and will return a `1` iff `n` is odd)! BUT we need to be careful to make sure ONLY ONE THREAD adds in that extra value or we are going to overcount things now (hence the check for the `threadIdx.x == 0`)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1qJnIPw0PCt"
      },
      "outputs": [],
      "source": [
        "__global__\n",
        "void vectorReduction(int *d_vals, int n) {\n",
        "    while(n > 1){\n",
        "        for(int i = threadIdx.x; i < n/2; i += blockDim.x){\n",
        "            d_vals[i] += d_vals[i + n/2];\n",
        "        }\n",
        "        __syncthreads();\n",
        "        if (n%2 && threadIdx.x == 0){d_vals[0] += d_vals[n-1];}\n",
        "        __syncthreads();\n",
        "        n /= 2;\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJqLlSvLjZrN"
      },
      "source": [
        "## Branches and I/O Overhead\n",
        "Lets explore the slowdown caused by branches and I/O in the example below! What we'll do is time two pieces of code. The first has a branch. The second uses two steps without a branch. Do you notice a difference in their runtime? Why or why not? Note that this function does nothing as it just sets a temp but illustrates the timing point.\n",
        "\n",
        "Then lets include the I/O timing -- what do you see now? Does this surprise you?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFOQJjvSjZA9"
      },
      "outputs": [],
      "source": [
        "%%gpurun -n branch.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "#define time_delta_us_timespec(start,end) (1e6*static_cast<double>(end.tv_sec - start.tv_sec)+1e-3*static_cast<double>(end.tv_nsec - start.tv_nsec))\n",
        "\n",
        "// this function does nothing as it just sets a temp\n",
        "// but illustrates the timing point\n",
        "__global__\n",
        "void branchy(float *d_vals, int n) {\n",
        "    for (int j = 0; j < 100; j++){\n",
        "      float temp = 0;\n",
        "      for (int ind = threadIdx.x; ind < n; ind += blockDim.x){\n",
        "          if (ind % 2){temp += d_vals[ind];}\n",
        "          else{temp += 7*2 + d_vals[ind]/3 + 11;}\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void twoSteps(float *d_vals, int n) {\n",
        "    for (int j = 0; j < 100; j++){\n",
        "      float temp = 0;\n",
        "      for (int ind = 2*threadIdx.x; ind < n; ind += blockDim.x){\n",
        "          temp += d_vals[ind];\n",
        "      }\n",
        "      // this is not needed because we are only touching a thread local temp\n",
        "      // but/and including to penalize the two steps as much as possible!\n",
        "      __syncthreads();\n",
        "      for (int ind = 1 + 2*threadIdx.x; ind < n; ind += blockDim.x){\n",
        "          temp += 7*2 + d_vals[ind]/3 + 11;\n",
        "      }\n",
        "      __syncthreads();\n",
        "    }\n",
        "}\n",
        "\n",
        "__host__\n",
        "int main() {\n",
        "    const int n = 1000;\n",
        "    struct timespec start, end;\n",
        "\n",
        "    // initialize the arr to 0s and copy onto the GPU\n",
        "    float *h_vals = (float *)calloc(n,sizeof(float));\n",
        "    float *d_vals; cudaMalloc((void**)&d_vals, n*sizeof(float));\n",
        "    cudaMemcpy(d_vals, &h_vals, n*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // run the kernels once to warm up the GPU\n",
        "    branchy<<<1, n>>>(d_vals, n);\n",
        "    twoSteps<<<1, n>>>(d_vals, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // time the kernels\n",
        "    clock_gettime(CLOCK_MONOTONIC,&start);\n",
        "    branchy<<<1, 100>>>(d_vals, n);\n",
        "    cudaDeviceSynchronize();\n",
        "    clock_gettime(CLOCK_MONOTONIC,&end);\n",
        "    printf(\"Branchy time: %f us\\n\",time_delta_us_timespec(start,end));\n",
        "\n",
        "    clock_gettime(CLOCK_MONOTONIC,&start);\n",
        "    twoSteps<<<1, 100>>>(d_vals, n);\n",
        "    cudaDeviceSynchronize();\n",
        "    clock_gettime(CLOCK_MONOTONIC,&end);\n",
        "    printf(\"TwoSteps time: %f us\\n\",time_delta_us_timespec(start,end));\n",
        "\n",
        "    // copy the result back\n",
        "    clock_gettime(CLOCK_MONOTONIC,&start);\n",
        "    cudaMemcpy(&h_vals, d_vals, n*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaDeviceSynchronize(); // redundant but adding for clarity/safety\n",
        "    clock_gettime(CLOCK_MONOTONIC,&end);\n",
        "    printf(\"Pure I/O time: %f us\\n\",time_delta_us_timespec(start,end));\n",
        "\n",
        "    free(h_vals); cudaFree(d_vals);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR2VlpQ7r1Cc"
      },
      "source": [
        "Note that if you run this a few times you'll likely see slightly different numbers because the GPU is running on a shared cloud server and these times are only directionally correct.\n",
        "\n",
        "If you want to see something really interesting comment out the two kernel calls under the comment:\n",
        "\n",
        "```\n",
        "// run the kernels once to warm up the GPU\n",
        "```\n",
        "\n",
        "What happens to the timing?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC02pqwnsQdT"
      },
      "source": [
        "## Advanced Synchronization (Atomics)\n",
        "Now imagine you are writing a CUDA program to increment a value `n` times in parallel. In this example we neglect to leverage any synchronization mechanisms. Run the code below to see what happens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHNFhR1lsISL"
      },
      "outputs": [],
      "source": [
        "%%gpurun -n atomic.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__\n",
        "void incrementValue(int *d_val, int n) {\n",
        "    for (int tid = threadIdx.x; tid < n; tid += blockDim.x){\n",
        "        *d_val += 1;\n",
        "    }\n",
        "}\n",
        "\n",
        "__host__\n",
        "int main() {\n",
        "    const int n = 1000;\n",
        "\n",
        "    // initialize the value to 0 and copy onto the GPU\n",
        "    int h_val = 0;\n",
        "    int *d_val; cudaMalloc((void**)&d_val, sizeof(int));\n",
        "    cudaMemcpy(d_val, &h_val, sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // run the kernel\n",
        "    incrementValue<<<1, n>>>(d_val, n); // Launch kernel with n threads\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // copy the result back\n",
        "    cudaMemcpy(&h_val, d_val, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // display the result\n",
        "    std::cout << h_val << \" \";\n",
        "\n",
        "    cudaFree(d_val);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG3cv5crt8yJ"
      },
      "source": [
        "Yikes! That's a problem! **What if we instead use some atomics?** Note that all we are changing is the `+=` into an `atomicAdd`! You can think of an atomic as a local sync on a single piece of data to protect it during multi-threading (and we'll discuss next week why we need this from a computer systems/architecture perspective)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUaRP-Qys_KW"
      },
      "outputs": [],
      "source": [
        "%%gpurun -n atomic2.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__\n",
        "void incrementValueAtomic(int *d_val, int n) {\n",
        "    for (int tid = threadIdx.x; tid < n; tid += blockDim.x){\n",
        "        atomicAdd(d_val,1);\n",
        "    }\n",
        "}\n",
        "\n",
        "__host__\n",
        "int main() {\n",
        "    const int n = 1000;\n",
        "\n",
        "    // initialize the value to 0 and copy onto the GPU\n",
        "    int h_val = 0;\n",
        "    int *d_val; cudaMalloc((void**)&d_val, sizeof(int));\n",
        "    cudaMemcpy(d_val, &h_val, sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // run the kernel\n",
        "    incrementValueAtomic<<<1, n>>>(d_val, n); // Launch kernel with n threads\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // copy the result back\n",
        "    cudaMemcpy(&h_val, d_val, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // display the result\n",
        "    std::cout << h_val << \" \";\n",
        "\n",
        "    cudaFree(d_val);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "djeJUiRynMtn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}